---
title: "ANALYSING COUNT DATA"
author: 
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
    social: menu
    source_code: embed
runtime: shiny
---
Probability mass functions
===

```{r global, include=FALSE}

  library(MASS)  # for neg binomial analysis and correlated data generation
  library(ggplot2)
  library(lme4)
  library(shiny)
  library(utf8)  # codes for Greek letters
  library(MethComp)
  library(gridExtra)
  library(tidyverse)
  library(car)  # diagnostics
  library(vcd)  # diagnostics
  lwd.=3  # used when plotting

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# function for nice upper limits to count bar plots, stack exchange
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
    roundUpNice <- function(x, nice=c(1,1.5, 2,4,5,6,8,10)) { #added 1.5 to help bar plot y scale
    if(length(x) != 1) stop("'x' must be of length 1")
    10^floor(log10(x)) * nice[[which(x <= 10^floor(log10(x)) * nice)[[1]]]]
  }
  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the Negative Binomial probability density function, with parameters
# mu and alpha. The mean of this NB distribution is mu, and variance is sigma^2=mu+alpha*mu^2
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
   e_dnbinom=function(x,mu,alpha){
    m = mu
    k = 1/alpha
    # lgamma is log(gamma)
    a = lgamma(k+x)-lgamma(k)-lgamma(x+1)
    b = x*log(m/(m+k))
    c = -k*log(1+m/k)
    prob = exp(a+b+c)
    prob[x<0] = 0
    prob[prob>1] = 1
    prob[prob<0] = 0
    return(prob)
  }

# check
# dnbinom(   1, size=2,   mu=3)
# e_dnbinom(1, alpha=1/2,mu=3)

##################################################################################
# R has built-in random number generators for the binomial distribution,
# but they are expressed in different form than the form in the Lloyd-Smith
# PLoS paper. This function translates the mu and alpha parameters
# into the format needed by R to generate the NB random numbers
##################################################################################
    
  e_rnbinom=function(n,mu,alpha){
    size = 1/alpha          # so now we can just enter alpha this is converted to 1/ alpha
    prob = size/(mu+size)   # so now we can enter mu the mean and that is converted to prob
    return(rnbinom(n,size=size,prob=prob))
  }

 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # generate cor data function, see references
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  GenerateMultivariatePoisson<-function(p, samples, R, lambda){
    normal_mu=rep(0, p)
    normal = mvrnorm(samples, normal_mu, R)
    unif=pnorm(normal)
    pois=t(qpois(t(unif), lambda))
    return(pois)
  }
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # another function to generate cor data , see references
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  mvrnorm <- function(n = 1, mu = 0, Sigma) {
    nvars <- nrow(Sigma)
    # nvars x n matrix of Normal(0, 1)
    nmls <- matrix(rnorm(n * nvars), nrow = nvars)
    # scale and correlate Normal(0, 1), "nmls", to Normal(0, Sigma) by matrix mult
    # with lower triangular of cholesky decomp of covariance matrix
    scaled_correlated_nmls <- t(chol(Sigma)) %*% nmls
    # shift to center around mus to get goal: Normal(mu, Sigma)
    samples <- mu + scaled_correlated_nmls
    # transpose so each variable is a column, not
    # a row, to match what MASS::mvrnorm() returns
    t(samples)
  }
  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# function to simulate negative binomial RCT
# rnbinom simulation accounting for trt discontinuation, outputs p value and other objects
# enter one arm n, k, placebo rate, trt effect, drop off trt rates and follow up time
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
  nb.power <- function(n=220, k=1.3, mu0=1, eff=0.65, drop1=.1, drop2=.1, fup=1) {
    
    # n=220; alpha=1.3; mu0=1; mu1=.65; drop1=.01; drop2=.01; fup=1; k=.8
    
    mu1 <- eff*mu0 # manifest the true trt effect
    
    # just so there is no error if drop out or k=0
    if (drop1==0) {drop1=0.0001}
    if (drop2==0) {drop2=0.0001}
    if (k==0) {k=0.0001} 
  
    dose <- c(rep("placebo",n),rep("trt",n)) # 50:50 split of patients
    
    mu   <- c(rep(mu0,n), rep(mu1,n))        # rates in two arms
  
    drop <- c(rep(drop1,n), rep(drop2,n))    # tr discontinuation rates
    
    f <- - rexp(2*n) / log(1-drop/fup)       # generate drop outs and scale time according to follow up!
    
    indiv.time <- ifelse(f > fup, fup, f)       # curtail at follow up time 
    
    y  <-  rnbinom(n*2,  prob=1/(1+ mu*indiv.time* k),        size=1/k)  +   # on treatment events
           rnbinom(n*2,  prob=1/(1+ mu0*(fup-indiv.time)*k),  size=1/k)      # accounting for any off treat. 
    
    # assume no one is lost to the study but can discontinue treatment
    logtime  <- rep(log(fup), n*2)          # make sure follow up here !
    
    # analyse with neg. binomial model
    mod <- glm.nb(y~dose+offset((logtime)))
    
    x <- summary(mod)
   
    # collect p-values
    p <-  x$coefficients["dosetrt","Pr(>|z|)"]
     
    return(list(p, y ,dose, x, mod, logtime))
    
  } 


    # example1 <-  nb.power(n=220, k=1.3, mu0=1, eff=.65, drop1=.1, drop2=.1, fup=1)
    # barplot(unlist(table(example1[2])))
    # 
    # res <- replicate( 1000, 
    #      nb.power(n=220, k=1.3, mu0=1, eff=.65, drop1=.1, drop2=.1, fup=1))
    # mean(unlist(res[1,]<.05))


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# function to simulate negative binomial RCT
# Poisson gamma simulation accounting for trt discontinuation, outputs p value and other objects
# enter one arm n, k, placebo rate, trt effect, drop off trt rates and follow up time
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
nb.power2 <- function(n=220, k=1.3, mu0=1, eff=0.65, drop1=.1, drop2=.1, fup=1) {
  
  mu1 <- eff*mu0
  
  # n=220; alpha=1.3; mu0=1; mu1=.65; drop1=.1; drop2=.1; fup=1; k=.8
  # just so there is no error if drop out is or k=0
  
  if (drop1==0) {drop1=0.0001}
  if (drop2==0) {drop2=0.0001}
  if (k==0) {k=0.0001} 
  
  dose <- c(rep("placebo",n),rep("trt",n)) # 50:50 split of patients
  
  mu   <- c(rep(mu0,n), rep(mu1,n))        # rates in two arms
  
  drop <- c(rep(drop1,n), rep(drop2,n))    # tr discontinuation rates
  
  f <- - rexp(2*n) / log(1-drop/fup)       # generate drop outs and scale time according to follow up!
  
  indiv.time <- ifelse(f > fup, fup, f)    # curtail at follow up time 

  s <-  rgamma(2*n, shape = 1/k, scale = k)
  
  # lambda1 <- mu   * s  # Simulate rate
  # y <-   rpois(n = n*2, lambda = (indiv.time*lambda1))  +
  #        rpois(n = n*2, lambda = ((fup-indiv.time)*lambda1))  # ensure follow up here!
  
  y <- rpois(n = n*2, lambda = (indiv.time*mu*s))  +
       rpois(n = n*2, lambda = ((fup-indiv.time)*mu0*s))  # ensure follow up here!
  
  # assume no one is lost to the study but can discontinue treatment
  logtime  <- rep(log(fup), n*2)          # make sure follow up here !
  
  # analyse with neg. binomial model
  mod <- glm.nb(y~dose+offset((logtime)))
  x <- summary(mod)
  
  # collect p-values
  p <-  x$coefficients["dosetrt","Pr(>|z|)"]
  
  return(list(p, y ,dose,x,mod, logtime))
  
}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# function to generate correlated poisson and analyse and evaluate
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

po.cor.power <- function(n=22, r=.75, mu0=10, eff.p=.75) { # r correlation mu0 placebo rate, mu1 expected change in rate
   
  #n=22; r=.75; mu0=10; eff.p=.75
  # 1. create correlated poisson
  # 2. analyse many times and examine

  L1 <- mu0
  L2 <- mu0*eff.p
  
  # generate correlated poisson ref: https://thomasward.com/simulating-correlated-data/
  # Sample correlated N(0, 1) distributions from a multivariate normal distribution.
  # Transform them to correlated Uniform(0, 1) distributions with the normal CDF.
  # Transform them to any correlated probability distribution you desire with that probability distribution’s inverse CDF.
  
  Sigma <- matrix(c(1, r, r, 1), 2, 2)
 
  p2 <- mvrnorm(n, Sigma = Sigma)   # correlated continuous, see function in global area!
  
  U <- pnorm(p2, mean = 0, sd = 1)  # correlated uniform
  pp1 <- qpois(U[, 1], L1)          # correlated poisson
  pp2 <- qpois(U[, 2], L2)          # correlated poisson
  
  cor. <- cor(pp1,pp2)              # capture correlation
  
  # create a data frame
  my_data <- data.frame( 
    group = rep(c("A.before", "B.after"), each = n),
    counts = c(pp1,  pp2),
    ID=rep(1:n,2)
  )
  
  # mixed model
  A <- glmer(counts ~ group + (1|ID), data=my_data, family="poisson")
  
  # B <-    glmer(counts ~ 1     + (1|ID), data=my_data, family="poisson")  # dont use LRT to speed up sims
  # p <-    anova(A,B)
  # mix <-  p$`Pr(>Chisq)`[2]
  
  x <- summary(A)
  mix <- summary(A)$coeff[2,"Pr(>|z|)"]  # to speed up simulations we don't use LR test but Wald test
  
  rate.reduction <- exp(x$coefficients[2,1])
  
  intercept <- exp( x$coefficients[1,1])
  beta      <- exp( x$coefficients[2,1])
  
  x1 <- pp2 - pp1                                     # post - pre
  
  w <- wilcox.test(x=x1, paired=FALSE, correct=FALSE, conf.int=TRUE  ,conf.level = 0.95)
   
  wil <- w$p.value
  wile <- w$estimate[1][[1]]
  
  t.t <- t.test(x=x1, paired=FALSE)$p.value # t.test(x1)$p.value 
   
  signed_rank = function(x) sign(x) * rank(abs(x))
  ttor <- t.test(signed_rank(x1))$p.value
  
  # wilcox.test(x1)
  # t.test(signed_rank(x1))
  
  #lets capture SD of differences
  sdd <- sd(x1)
  
  #newList <- list("glmer" = mix , "Wilcoxon signed rank test" = wil,
              #    "t.test"=t.t, "rate reduction"= rate.reduction, "intercept"=intercept, "beta"=beta,
               #   "ttest on ranks"=ttor, "sd of diff"=sdd)
  #return(newList)
   # collect all relevant info
   c(rate.reduction,mix , wil,t.t,  ttor,intercept, beta ,sdd, wile, cor.)
  
}


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# start of app
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```

Column {.sidebar}
-----------------------------------------------------------------------

**Count data analysis is explored**

```{r}


  sliderInput('mu', 'Common  \u03BC', value=1.5,
                min = 1, max = 25, step=.5, ticks=F)
  
  sliderInput('alpha_a', '\u03B1 for red curve', value = c(0.8),
                min = 0, max = 5 ,step=0.01, ticks=F)
  
  sliderInput('alpha_b', '\u03B1 for dark blue curve', value = c(0.8),
                min = 0, max = 5 ,step=0.01, ticks=F)
  
  sliderInput('alpha_d', '\u03B1 for purple curve', value = c(0.8),
                min = 0, max = 5 ,step=0.01, ticks=F)
  
  sliderInput('sims', 'Simulations (chart 2)', 50000,
                min = 50000, max = 500000,step=50000,ticks=F)
  
  sliderInput('x_range', 'Plot x-range', value = c(0,10),
          min = 0, max = 80,step=5,ticks=F)
  
  sliderInput('y_range', 'Plot y-range', value = c(0,.4),
          min = 0, max = 1,step=.05,ticks=F)

  checkboxInput('perc','show stats on barplot', value=TRUE)

```

This page of the app uses  $\alpha$. 

$\alpha$ takes on positive rational values, rarely above 4 (Hilbe page 190)

When  $\alpha$ approaches 0, k is infinite and Poisson emerges (move red $\alpha$ slider to 0).

When  $\alpha$ > 1, k < 1 highly leads to over dispersed data ($\alpha$ big).

$\alpha$ = 1/k

k = 1/ $\alpha$
 

Column {data-width=400, height=300}
-----------------------------------------------------------------------
### Chart 1


```{r}

renderPlot({


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# NB density function, and overlay the Poisson distribution with the same mean
# Normal with variance equal to mean also
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
  # mu = sample(1:25,1)
  # alpha_a = 0.1
  # alpha_b = .01
  # alpha_d = 1

  mu =      input$mu
  alpha_a = input$alpha_a
  alpha_b = input$alpha_b
  alpha_d = input$alpha_d
  low =     input$x_range[1]
  high=     input$x_range[2]
  xmax =    high
  ymax =    input$y_range[2]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  # if else to turn neg binomial into poisson if alpha =0
  x=c(-0.0001,seq(0,xmax,1))
  
  y =          dpois(x,mu)
   
  if(alpha_a==0) {
  yb =          dpois(x,mu)
  }else{
  yb=     e_dnbinom(x,mu,alpha_a)
  }
  
  if(alpha_b==0){
  yc =          dpois(x,mu)
  }else{
  yc=     e_dnbinom(x,mu,alpha_b)
  }
  
  if(alpha_d==0){
  yd =          dpois(x,mu)
  }else{
  yd=     e_dnbinom(x,mu,alpha_d)
  }
  
  nor <-       dnorm(x, mu, sqrt(mu))
 
  x=  c(0,x)  
  y=  c(0,y)
  yb= c(0,yb)
  yc= c(0,yc)
  yd= c(0,yd)
  nor=c(0,nor)
  
  l = !is.na(y+yb+yc+yd+nor)
  
  plot(x[l],y[l], ylim=c(0,ymax), xlim=c(low,xmax), type="l", lwd=lwd., xlab="x",
       ylab="prob(x)", main="Probability distributions")
  
  lines(x[l],yb[l], col=2,lwd=lwd.,type="l")
  lines(x[l],yc[l], col=4,lwd=lwd.,type="l")
  lines(x[l],nor[l],col=5,lwd=lwd.,type="l")
  lines(x[l],yd[l], col=6,lwd=lwd.,type="l")
   
  apois = paste("Poisson \u03BC=", mu,sep="")
  nb_a = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_a,sep="")
  nb_b = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_b,sep="")
  nb_c = paste("Normal \u03BC=",   mu," \u03C3=sqrt(",mu,")",sep="")
  nb_d = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_d,sep="")
  
  legend("topright",col=c(1,2,4,6,5),legend=c(apois, nb_a, nb_b, nb_d, nb_c), lwd=lwd.,bty="n")
 }
)

```


### Chart 2

```{r}

renderPlot({
  
    n=input$sims
    
    mu = input$mu
    alpha_a =  input$alpha_a
    alpha_b =  input$alpha_b
    alpha_d =  input$alpha_d
    low =      input$x_range[1]
    xmax=      input$x_range[2]
    ymax =     input$y_range[2]
    
    z  =      rpois(n,mu)
    zb = e_rnbinom(n,mu,alpha_a)
    zc = e_rnbinom(n,mu,alpha_b)
    zd =      rnorm(n,mu,sqrt(mu))
    ze = e_rnbinom(n,mu,alpha_d)
    
    a=hist(z, plot=F,breaks=seq(-1.5,n+.5,1))
    b=hist(zb,plot=F,breaks=seq(-1.5,n+.5,1))
    c=hist(zc,plot=F,breaks=seq(-1.5,n+.5,1))
    d=hist(zd,plot=F,breaks=200)
    e=hist(ze,plot=F,breaks=seq(-1.5,n+.5,1))
    
    atitle = paste("Distributions of",n,"random numbers")
    
    plot(a$mids,a$density,type="s",lwd=lwd., col=1, ylim=c(0,ymax),
         main=atitle, xlim=c(low,xmax),
         xlab="x", ylab="Fraction falling within each bin")
    
    lines(b$mids,b$density,type="s",lwd=lwd.,col=2)
    lines(c$mids,c$density,type="s",lwd=lwd.,col=4)
    lines(d$mids,d$density,type="s",lwd=lwd.,col=5)
    lines(e$mids,e$density,type="s",lwd=lwd.,col=6)
    
    apois =       paste("Poisson \u03BC="  ,mu,  sep="")
    nb_a = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_a,     sep="")
    nb_b = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_b,     sep="")
    nb_c = paste("Normal \u03BC=",   mu," \u03C3=sqrt(",mu,")", sep="")
    nb_d = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_d,     sep="")
    
    legend("topright",col=c(1,2,4,6,5),legend=c(apois, nb_a, nb_b, nb_d, nb_c),
           lwd=lwd.,bty="n")


})

```

Column {data-width=400}
-------------------------------------

### Chart 3

```{r}

renderPlot({
  
  mu = input$mu
  alpha_a = input$alpha_a
  low =  input$x_range[1]
  xmax=  input$x_range[2]
  ymax = input$y_range[2]
  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  xx <- c(seq(0,xmax,1))

  yb = dpois(xx,mu)
  
  par(mar=c(2.1,4.1,4.1,4.1))
      
  b <- barplot(yb, xlim=c(0,xmax+2), ylab = "Percent", xlab="x", col='black', ylim=c(0,ymax),
          main = paste("Poisson \u03BC=", mu, ", based on common \u03BC",sep=""),
          names.arg= as.character(xx))
   
  if(input$perc) {
  x <-0:length(b) #  % onto top of bars
  y1 <- yb + 0.02
  z <- round(yb,2) *100
  z <- paste0(z,"%")
  text(x=b[x+1],y=(y1),labels=z, cex = 1)
  }

})

```

### Chart 4

```{r}

    renderPlot({
  
    mu = input$mu
    alpha_a = input$alpha_a
  
    low =  input$x_range[1]
    xmax=  input$x_range[2]
    ymax = input$y_range[2]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  xx <- c(seq(0,xmax,1))

  if(alpha_a==0) {
  yb =          dpois(xx,mu)
  }else{
  yb=     e_dnbinom(xx,mu,alpha_a)
  }
  
  par(mar=c(2.1,4.1,4.1,4.1))
      
  b <- barplot(yb, xlim=c(0,xmax+2), ylab = "Percent", xlab="x", col='red',
          ylim=c(0,ymax),
          main =  paste("Negative Binomial \u03BC=",mu," \u03B1=",alpha_a,", based on common mean and red curve \u03B1",sep=""),
          names.arg= as.character(xx))
  
  if(input$perc) {
  x <-0:length(b) # % onto top of bars
  y1 <- yb + 0.02
  z <- round(yb,2) *100
  z <- paste0(z,"%")
  text(x=b[x+1],y=(y1),labels=z, cex = 1)
  }
})

```
 

Gamma Poisson power
====

Column {.sidebar}
-----------------------------------------------------------------------

Use the inputs below to simulate an RCT with a count outcome measure. We estimate the treatment effect using a gamma Poisson model. Be careful entering dispersion aka heterogeneity (ancillary) parameter. For example we may quote a negative binomial dispersion parameter of 0.8, referring to alpha, but we need to enter k or 1/alpha, so 1/0.8 = ~1.3 which is k.

```{r}
 
 
# 220
# 1
# .75
# 1.3
# .1
# .1
# 1
# talk about a neg binomial dispersion parameter of 0.8, so this is alpha but then enter 1/.8 = ~1.3 so this is k

    sliderInput('N', 'Patients per arm', value=190,
                  min = 10, max = 1000, step=5, ticks=F)

    sliderInput('mu0', 'Placebo event rate \u03BC per patient yr of follow up', value=1.5,
                  min = 0.1, max = 25, step=.05, ticks=F)

    sliderInput('eff', 'Hypothesised treatment effect ', value=.6, 
                  min = 0.1, max =5, step=.05, ticks=F)

    sliderInput('k', 'This needs to be k the heterogeneity (ancillary) parameter. k = 1/\u03B1', value = c(1.3),
                  min = 0, max = 5 ,step=0.01, ticks=F)

    sliderInput('d1', 'Placebo discontinuation prob', value = c(0.15),
                  min = 0, max = .5 ,step=0.05, ticks=F)

    sliderInput('d2', 'Treatment discontinuation prob', value = c(0.15),
                  min = 0, max = .5 ,step=0.05, ticks=F)

    sliderInput('fup', 'Follow up (yrs)', value = c(1),
            min = 1, max = 10, step=1,ticks=F)

    sliderInput('sims2', 'Power simulations', value= 500,
                  min = 500, max = 10000, step= 500, ticks=F)
    
    checkboxInput('perc1','show stats on barplot', value = TRUE)

```

Row {data- height=650}
-----------------------------------------------------------------------
### Power will appear here imminently...be patient...
  
```{r, eval=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# large simulations of rnbionom approach

r1<- reactive({
  
  res <- replicate( input$sims2, 
        nb.power(n=input$N, k=input$k, mu0=input$mu0, eff=input$eff, drop1=input$d1, drop2=input$d2, fup=input$fup)) 
  mean(unlist(res[1,]<.05))

})
 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# large simulations of gamma Poisson approach

r2 <- reactive({
  
  res1 <- replicate( input$sims2, 
        nb.power2(n=input$N, k=input$k, mu0=input$mu0, eff=input$eff, drop1=input$d1, drop2=input$d2, fup=input$fup)) 
  mean(unlist(res1[1,]<.05))

})

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

reactive({
  
cat(paste0("A RCT with 1:1 randomisation is being planned to investigate an intervention on the rate of asthma exacerbations with ",input$N," patients per arm.\n"))

cat(paste0("The gamma Poisson model will be used as we expect patient heterogeneity of exacerbations beyond that captured by patient level covariates.\nWe will use the natural log of duration of follow up time as an offset variable, the duration of follow up ",input$fup, " will be the planned duration of follow up for all patients."))

cat(paste0("\nk is the dispersion parameter, its value is " ,input$k,", this is the same as 1/alpha So alpha is ",1/input$k,". The rate for placebo is ",input$mu0," events per patient year of follow up.\nThe treatment effect hypothesised is ",input$eff, " so an event rate for treatment is hypothesised as ",input$mu0*input$eff, " events per patient year follow up. \nDrop out probabilities for treatment discontinuation and placebo over the follow up period (in years) are " ,input$d1, " and " ,input$d2," respectively. \nWe assume a constant exponential hazard rate over time for treatment discontinuation with ",input$d1," and ",input$d2,", the percentages of patients\noff treatment by follow up time in years in active and placebo. No further treatment effect versus placebo is assumed during off treatment, that is ",input$mu0," events per patient year is \nassumed for patients after treatment discontinuation in treated and placebo.\n"))

cat(paste0("\nThe power estimate is based on simulation using rnbinom...",r1()," and ",r2()," based on Poisson gamma simulation"))
})
  

```
 
Row {data-height=700}
-------------------------------------
   
### A typical dataset that can be expected

```{r}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# single simulation of rnbinom approach

r3 <- reactive({
  
  example1 <-  nb.power(n=input$N, k=input$k, mu0=input$mu0, eff=input$eff, drop1=input$d1, drop2=input$d2, fup=input$fup)
  mu1 <- input$eff*input$mu0
 return(list( y=example1[2][[1]], dose=example1[3][[1]] , model=example1[4][[1]]   , mu1=mu1)  )
  
})
 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 renderPlot({
  
    mu1 <-  r3()$mu1
    dose <- r3()$dose
    y <-    r3()$y
    
    k=     (input$k)
    mu0=   (input$mu0)
    
    statz <- (input$perc1)
   
    trt <- y[dose %in% "trt"]
    pla <- y[!dose %in% "trt"]
    
 #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  par(mfrow=c(2,2))
  # proportion plot
  u1 <- max(prop.table(table(trt)) , prop.table(table(pla)))*100
  upp <- roundUpNice( u1)

  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
  x <- factor(trt, levels=0:max(y))
  nn <- data_perc <- t(prop.table(table(x))) * 100    # Convert data to probability table
 
  b <- barplot(data_perc, ylab = "Percent", 
          main =paste0("Treated true \u03BC=",mu1," & k=",k," \nso \u03B1=",
                       round(1/k,2),""), ylim=c(0,upp+5 ), col='blue')  #
  
  x <-as.numeric(colnames(nn))  # % onto top of bars
  y1 <- as.vector(nn)+4
  z <- as.character(as.vector(nn))
  z <-  round(as.numeric(z),0)
  z <- paste0(z,"%")
  
  if(isolate(statz)) { 
    text(x=b[x+1],y=(y1),labels=z, cex = 1, col='black') }  
          
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
  
  x <- factor(pla, levels=0:max(y))
  nn <- data_perc <- t(prop.table(table(x))) * 100    # Convert data to probability table
  b <-barplot(data_perc, ylab = "Percent",
          main =paste0("Placebo true \u03BC=",mu0," & k=",k," \nso \u03B1=",
                       round(1/k,2),""), ylim=c(0,upp+5 ), col='green') 
  
  if(isolate(statz)) { 
      
      x <-as.numeric(colnames(nn))   #  % onto top of bars
      y1 <- as.vector(nn)+4
      z <- as.character(as.vector(nn))
      z <-  round(as.numeric(z),0)
      z <- paste0(z,"%")
      text(x=b[x+1],y=(y1),labels=z, cex = 1)
  
    } 
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # count plot
   u <- ceiling(max(table(y))/2)
   u <- roundUpNice(u)
 
  #get the midpoints in bars with b object
  b<- barplot(table(factor(trt, levels=0:max(y)))  , main ="Treated counts" , col='blue',   ylim=c(0,u+25)) #u+ to allow text
  
    if(isolate(statz)) { 
       
      nn <- table(factor(trt))  # counts onto top of bars
      x <-as.numeric(names(nn))
      y1 <- as.vector(nn)+10
      z <- as.character(as.vector(nn))
      text(x=b[x+1],y=(y1),labels=z, cex = 1)  # x+1 th of b vector because bars start at zero 
    }
    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

  b <-barplot(table(factor(pla, levels=0:max(y)))  , main ="Placebo counts" , col='green',  ylim=c(0,u+25 ))
  
     if(isolate(statz)) { 
        
      nn <- table(factor(pla))  # counts onto top of bars
      x <-as.numeric(names(nn))
      y1 <- as.vector(nn)+10
      z <- as.character(as.vector(nn))
      text(x=b[x+1],y=(y1),labels=z, cex = 1)
    }
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  par(mfrow=c(1,1))
    
})

```   
 
   
### A typical result that can be expected, here theta is synonymous with $\alpha$

```{r}

 renderPrint({
  
   f <- r3()$model
   print(f)

  cat(paste0("\nEstimated placebo rate ",
             round(exp(f$coef[1]),4)," and multiplicative treatement effect estimate of ",round(exp(f$coef[2]),4),""))
    
})
  
``` 

Gamma Poisson
====

Column {.sidebar}
-----------------------------------------------------------------------

Show the equivalence of gamma Poisson and negative binomial (rnbinom) parmetrisations ! Be careful entering dispersion aka heterogeneity (ancillary) parameter. For example we may quote a negative binomial dispersion parameter of 0.8, referring to alpha, but we need to enter k or 1/alpha, so 1/0.8 = ~1.3 which is k.

```{r}

    sliderInput('N.pg', 'Patients per arm', value=1000,
                  min = 1000, max = 100000, step=5, ticks=F)
    
    sliderInput('mu0.pg', 'Placebo event rate \u03BC per patient yr of follow up', value=1,
                  min = 0.1, max = 25, step=.05, ticks=F)
    
    sliderInput('eff.pg', 'Hypothesised treatment effect ', value=.75,  # 1/2 ing to doubling
                  min = 0.5, max = 2, step=.05, ticks=F)
  
    sliderInput('k.pg', 'This needs to be k the heterogeneity (ancillary) parameter. k = 1/\u03B1', value = c(1.3),
                  min = 0, max = 5 ,step=0.01, ticks=F)
    
    sliderInput('d1.pg', 'Placebo discontinuation prob', value = c(0.1),
                  min = 0, max = .5 ,step=0.05, ticks=F)
    
    sliderInput('d2.pg', 'Treatment discontinuation prob', value = c(0.1),
                  min = 0, max = .5 ,step=0.05, ticks=F)
  
    sliderInput('fup.pg', 'Follow up (yrs)', value = c(1),
            min = 1, max = 10, step=1,ticks=F)
     
    # n  <- 1000
    # k  <- 1.3   # this is k, alpha=1/k
    # 1/k         # alpha reported as theta in neg binomial
    # mu0 <- 1
    # mu1 <- mu0*0.75
    # 
    # fup   <- 1
    # drop1 <- 0.1
    # drop2 <- 0.1
        
```

The plots on this page are mostly from one model (data simulated gamma Poisson)  
 

Column {data-width=400, height=300}
-----------------------------------------------------------------------

### Gamma Poisson data simulation (theta is synonymous with $\alpha$)

```{r, eval=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# single simulation of gamma poisson

  r4 <- reactive({
    
   example1 <-  nb.power2(n=input$N.pg, k=input$k.pg, mu0=input$mu0.pg, eff=input$eff.pg,
                           drop1=input$d1.pg, drop2=input$d2.pg, fup=input$fup.pg)
    mu1 <- input$eff.pg*input$mu0.pg
   return(list( y=example1[2][[1]], dose=example1[3][[1]] , smodel=example1[4][[1]]   , mu1=mu1,  model=example1[5][[1]] ,
       L=example1[6][[1]]   ))
    
  })
 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  renderPrint({
   
     f <- r4()$smodel
     print(f)
      
  })

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``` 

### Negative binomial (rnbinom) data simulation (theta is synonymous with $\alpha$)


```{r, eval=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# single simulation using rnbinom

  r5 <- reactive({
    
    example1 <-  nb.power(n=input$N.pg, k=input$k.pg, mu0=input$mu0.pg, eff=input$eff.pg,
                           drop1=input$d1.pg, drop2=input$d2.pg, fup=input$fup.pg)
    mu1 <- input$eff.pg*input$mu0.pg
   return(list( y=example1[2][[1]], dose=example1[3][[1]] , smodel=example1[4][[1]]   , mu1=mu1, model=example1[5][[1]] ,
       L=example1[6][[1]]   ))
    
  })
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  renderPrint({
   
     f <- r5()$smodel
     print(f)
      
  })

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


```
   
Column {.tabset .tabset-fade}
-----------------------------------------------------------------------

### Rootograms
  

```{r, eval=TRUE}


 renderPlot({
   
 f1 <- r4()$model  # gamma Poisson approach simulation
 f  <- r5()$model  # rnbiom approach simulation
   
 par(mfrow=c(1,2))
 countreg::rootogram(f1, style = "hanging",     main = "Hanging plot based on gamma poisson simulation")
 countreg::rootogram(f,  style = "hanging",     main = "Hanging plot based on rnbinom simulation ")
 par(mfrow=c(1,1))
 
 })


```

###  A distribution that fits well is looked for...
 
 
```{r}

 renderPlot({
   
   f1 <- r4()   # gamma Poisson approach simulation
   
   Ord_plot(f1$y)
 
 })


```

### Poissoness?
  

```{r, eval=TRUE}

 renderPlot({
   
   f1 <- r4()   # gamma Poisson approach simulation
  
   distplot(f1$y, type="poisson")
 
 })


```

### Negative Binomialness?  
 
 
```{r}

 renderPlot({
     
     f1 <- r4()  # gamma Poisson approach simulation
      
     distplot(f1$y, type="nbinom")
   
   })

```

### Default diagnostic plots
 
 
```{r}

   renderPlot({
     
     f1 <- r4()$model # gamma Poisson approach simulation
     
     op <- par(mfrow=c(2,2), mar=c(4,4,2,1)+.1, cex.lab=1.2)
    
     plot(f1)
     
     par(op)
   
  
   })
   

```

### Quantile quantile and half normal plots

```{r, eval=TRUE}

 renderPlot({
   
   # gamma Poisson approach simulation
   f       <- r4()$model
   dose    <- r4()$dose
   y       <- r4()$dose
   logtime <- r4()$L

  par(mfrow=c(1,2))

  qqPlot(rstudent(f), xlab="Normal quantiles", ylab="Studentized residuals")

# ref: michael friendly
# Figure 11.38: Half-normal QQ plot of studentized residuals for the NB model fit 
# The reference line and confidence envelope reflect the mean and (2.5%, 97.5%) quantiles of
# the simulation distribution under the negative-binomial model for the same data

  observed <- sort(abs(rstudent(f)))
  
  n <- length(observed)
  
  expected <- qnorm((1:n + n - 1/8)/(2*n + 1/2))
  
  S <- 100
  sims <- simulate(f, nsim=S)
   
  resids <- function(y)
             rstudent(glm.nb(y ~ dose + offset(logtime),
  start=coef(f)))
  
  z <- apply(sims,2,resids)
  z <- abs(z)
  z <- apply(z, 2, sort)
 

 lim <- 0.95
 mean <- apply(z, 1, mean)
 lower <- apply(z, 1, quantile, prob=(1 - lim)/2)
 upper <- apply(z, 1, quantile, prob=(1 + lim)/2)

 plot(expected, observed,
   xlab='Expected value of half-normal order statistic',
   ylab='Absolute value of studentized residual')
   lines(expected, mean, lty=1, lwd=2, col="blue")
   lines(expected, lower, lty=2, lwd=2, col="red")
   lines(expected, upper, lty=2, lwd=2, col="red")
   
par(mfrow=c(1,1))

# res <- rstudent(f)
# plot(density(res), lwd=2, col="blue", 
# main="Density of studentized residuals")
# rug(res)
# 
#  # why the bimodality?
#  plot(jitter(log(y+1), factor=1.5), res,
#  xlab="log (articles+1)", ylab="Studentized residual")

})

```

Poisson paired counts
====

Column {.sidebar}
-----------------------------------------------------------------------


Use the inputs below to generate correlated count data !

```{r}


  sliderInput('N.p', 'Patients per arm', value=22,
                min = 10, max = 200, step=5, ticks=F)
  
  sliderInput('mu0.p', 'Placebo mean rate \u03BC', value=10,
                min = 0.1, max = 25, step=.05, ticks=F)
  
  sliderInput('eff.p', 'Hypothesised treatment effect ', value=.75,  # 1/2 ing to doubling
                min = 0.5, max = 2, step=.05, ticks=F)
  
  sliderInput('r', 'Correlation', value = c(.20),
                min = -1, max = 1 ,step=0.05, ticks=F)
   
  sliderInput('sims.p', 'Power simulations', value= 10,
                min = 100, max = 1000, step= 100, ticks=F)
 

```

Row {data-height=275}
-----------------------------------------------------------------------

### Single arm pre post study design simulation
  

```{r, eval=TRUE}

reactive({
  
cat(paste0("This is an example of paired study design in which the outcome measure is counts of events and which does not use a concurrent control, so is not a great study design. \nNevertheless, we have ",input$N.p," subjects assessed before and after an intervention. The Poisson model will be used, the observation time is the same for the the pre and post periods.\n"))

cat(paste0("We will simulate the data based on a expected rate before the intervention of ",input$mu0.p, " and an hypothesised effect of intervention ",input$eff.p,", so a hypothesised mean rate of ",input$mu0.p*input$eff.p," for the intervention.\nWe also take into account the correlation between obervations for the same individual, the starting value is " ,input$r,".\n")) 

cat(paste0("\nWe report the average p-value from a mixed Poisson model and the power, Wilcoxon signed rank test power, T test power, T test on ranks power, intercept, treatment effect and SD of differences. \nWe also quote the theoretical SD of differences."))

cat(paste0("\nTo generate correlated Poisson variables we..."))
cat(paste0("\n1) Sample two correlated N(0, 1) distributions from a multivariate normal distribution."))
cat(paste0("\n2) Transform them to correlated Uniform(0, 1) distributions using the normal CDF."))
cat(paste0("\n3) Transform them to the desired correlated probability distribution desired using that probability distribution’s inverse CDF!"))
 
})


``` 

Row {data-height=250}
-----------------------------------------------------------------------

### Results (Estimates and power)


```{r, eval=TRUE}

reactive({
  
  res <- replicate(input$sims.p, 
                   po.cor.power(n=input$N.p, r=input$r, mu0=input$mu0.p,  eff.p=input$eff.p ) )  

  x <- NULL
  x <- t((res))
  x <- as.data.frame(x)
  alpha=0.05
  
  R <- input$r# seq(-.9,.9,.1)   # correlation
  X <- input$mu0.p               # var of poisson (= mean)
  Y <- input$mu0.p*input$eff.p   # var of poisson (= mean)

  cat(paste0("Evaluations based on ",input$sims.p," simulations take into consideration correlation, the default starting value is "
       ,input$r,". The observed mean correlation is ",mean( unlist(x[,"V10"])), "\nThe median p-value from a mixed effects Poisson model is "
       ,median( unlist(x[,"V2"]))   , 
       ". \nPower based on the mixed model Wald test is ",
             
      (table( unlist(x[,"V2"])< alpha)/input$sims.p)[2][[1]],
      
      "\nPower based on the Wilcoxon signed rank test power is ",
      (table( unlist(x[,"V3"])< alpha)/input$sims.p)[2][[1]],
      "\nMean Hodeges Lehman estimator is ",
        mean( unlist(x[,"V9"]))," with 95%CI ",quantile(unlist(x[,"V9"]), .025)," to ",quantile(unlist(x[,"V9"]), .975),
      "\nPower based on the t-test is ",
      (table( unlist(x[,"V4"])< alpha)/input$sims.p)[2][[1]],
      "\nPower based on the t-test of ranks is ",
      (table( unlist(x[,"V5"])< alpha)/input$sims.p)[2][[1]],
      " \nThe mean intercept is ",
      mean( unlist(x[,"V6"])),
      "\nThe mean treatement effect is ",
      mean( unlist(x[,"V7"]))," with 95%CI ",quantile(unlist(x[,"V7"]), .025)," to ",quantile(unlist(x[,"V7"]), .975),
      "\nMean SD of differences is ", 
      mean( unlist(x[,"V8"])),  
    
      "\nThe theoretical SD of differences based on Poisson variances and stated correlation is ",
      sqrt(X+Y-(2*sqrt(X)*sqrt(Y)*R)),
      "\n"))
   

})

```

   
Row {.tabset .tabset-fade}
-----------------------------------------------------------------------

### Typical expected dataset (jitter added to counts)
  

```{r, eval=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   # use function
   dat2 <- reactive(
 
    GenerateMultivariatePoisson(p=2, samples=input$N.p, R=matrix(c(1, input$r, 
                                                                     input$r, 1), 2, 2), 
                                lambda=c(input$mu0.p, input$mu0.p*input$eff.p))
    )

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 renderPlot({   
   
  dat2 <- dat2()
  dx <- dat2

  # boxplot
  A <- dx[,1]
  B <- dx[,2]
  m <- max(A,B)
  before <- A + runif(length(dx[,1]),-.2,.2)
  after <-  B + runif(length(dx[,2]),-.2,.2)
  n <- length(before)
  d <- data.frame(y = c(before, after), 
                  x = rep(c(1,2), each=n),
                  id = factor(rep(1:n,2)))

  d$xj <- jitter(d$x, amount=.03)
  AA <- ggplot(data=d, aes(y=y) ) +
    geom_boxplot(aes(x=x, group=x), width=0.2, outlier.shape = NA) +
    geom_line(aes(x=xj, group=id),  colour='pink', alpha=.5) +
    geom_point(aes(x=xj), size=2) +
    xlab("Phase") + ylab("Count") +  
    scale_x_continuous(breaks=c(1,2), labels=c("Before", "After"), limits=c(0.5, 2.5)) +
     scale_y_continuous(breaks=c(0:m), limits=c(0,m)) +
    theme_bw() + theme(legend.position = "none") 
    
#~~~~~~~~~~~~~~~~~~~~~
     # scatter plot

     d <- data.frame(y = B ,
                     x = A)
   
      BB <- ggplot(d, aes(x, y)) +
      geom_jitter(width = 0.1, height = 0.1, size=2) +
      scale_x_continuous(breaks=c(0:m), limits=c(0,m)) +
      scale_y_continuous(breaks=c(0:m), limits=c(0,m)) +
      xlab("Before count") +ylab("After count") +
      theme_bw() + theme(legend.position = "none") +
        geom_abline(intercept=0, slope=1)

      plot1 <- AA
      plot2 <- BB
      grid.arrange(plot1, plot2, ncol=2)

})


```


### Typical expected dataset - Bland Altman plot, square root transformation
 
 
```{r}

 renderPlot({
   
    d <- dat2()

    my_data <- data.frame( 
      group = rep(c("before", "after"), each = input$N.p),
      counts = c(d[,1],  d[,2]),
      ID=rep(1:input$N.p,2)
    )

    m <- Meth(my_data,meth="group",item="ID",repl=NULL,y="counts",print=TRUE)
    
    BA.plot( m, model=NULL, repl.conn=TRUE, col.lines="blue",
             axlim=c(0,20), diflim=c(-20,20), xaxs="i", yaxs="i",
             las=1, eqn=FALSE, dif.type="lin", pl.type="BA", sd.type="lin",
             grid=1:9*10, digits=3,font.eqn=1 , Transform='sqrt') 

})

```

Wiki
=== 
**Notes: In general the app is quite slow as almost each page has some Monte Carlo simulation. But be patient eventually some information will appear.** The binomial distribution describes the number of r successes in n trials. The geometric distribution describes the number of failures before the first success, r. The negative binomial distribution describes the number of failures before the rth success. Confusingly, the term ‘dispersion parameter’ can refer to either k or $\alpha$ ; other terms  include ‘shape parameter’ 'heterogeneity (ancillary) parameter' and ‘clustering coefficient’. The only restriction placed on $\alpha$ is that it take positive rational values, rarely above 4 (according to Hilbe page 190). Underdispersed data are assigned the minimum value of $\alpha$, corresponding to k going to infinity. The dispersion parameter k is commonly used as an inverse measure of aggregation in biological count data. The larger the gamma shape parameter, the more dispersed is the distribution. The dispersion parameter k is commonly used as an inverse measure of aggregation in biological count data. When $\alpha$ = 1, the negative binomial distribution takes the form of a
geometric distribution, which is the discrete correlate of the continuous negative
exponential distribution. The negative binomial distribution with $\alpha$ = 0 is Poisson.
As the mean increases, the probability of a zero decreases, and the more the shape approximates a Gaussian distribution. As the Poisson mean gets larger it approaches normality, this is not the case for the negative binomial distribution. 

**Probability mass functions**
 
An exploration of the Poisson and the gamma Poisson distribution (aka negative binomial). A common mean can be adjusted and alpha the heterogeneity  dispersion parameter can be adjusted for each individual curve. When alpha is 0 we get the Poisson distribution (actually we use a vanishingly small value for alpha and not zero). Also see the normal distribution plotted for comparison. We use the density functions (top left) and the random number generators (top right). Below are bar graphs based on the density functions. 

**Gamma Poisson power**

An investigation of an analysis of count data and look at power for a 2 arm 1:1 randomised RCT, the outcome measure being a count.  The simulations are analysed using the the gamma Poisson model. The user can input the number of patients per arm, the expected mean rate on placebo and the hypothesised treatment effect.  For this model a heterogeneity parameter inputted as 1/alpha is required (based on historical data perhaps). We also allow patients to discontinue treatment, when they do they are assumed to continue to the end of study but their mean rate (if randomised to the new treatment) reverts to the placebo rate. We also have inputs for the follow up time and the number of simulations. An estimate of power is returned, as well as the results of a typical trial that can be expected. Power is given for both poisson gamma and rnbinom simulation approaches.
 
**Gamma Poisson**

A demonstration showing negative binomial and gamma Poisson are one and the same. Using the same adjustable population parameters one data set is simulated and analysed using the R rnbinom function and another data set simulated and analysed using the gamma and Poisson distributions. The gamma Poisson is a mixture of different Poisson distributions in which the response counts are assumed to follow a mixed Poisson distribution with a shared gamma distributed random subject effect. The tabs at the bottom present some diagnostic plots. 

The first plot tab shows hanging **Rootograms**. The counts are square rooted to approximately adjust for scale differences across the count levels. Otherwise, deviations would only be visible for levels with large observed/expected frequencies.

**'A distribution that fits well is looked for...'** on this tab we plot the number of occurrences against a certain frequency ratio (see Friendly (2000) for details) and should give a straight line if the data comes from a poisson, binomial, negative binomial or log-series distribution. The intercept and slope of this straight line conveys information about the underlying distribution. A OLS line (black) and a weighted OLS line (red) are shown. From the coefficients of the latter the distribution is estimated.   If none of the distributions fits well, no parameters are estimated.  A slope of 0 and + intercept suggest Poisson, a - slope and + interept suggest Binomial. ++ suggests Neg binomial and +- log series. 

**The plot tab 'Poissoness?'** If the Poisson distribution fits the data, the plot should show a straight line. **The plot tab by 'Negative Binomialness?'** If the Negative binomial distribution fits the data, the plot should show a straight line.

**Default diagnostic plots** for the gamma Poisson model fit are shown next. 
**Quantile quantile and half normal plots** quantile quantile (qq) plots and a more robust and useful version of qq plots: half normal plots, with simulated confidence intervals are next. The essential ideas are that Model departures and outliers are often easier to see for discrete data when the absolute values of residuals are plotted, because large positive and negative values are sorted together. This gives the half-normal plot, in which the absolute values of residuals, arranged in increasing order, |r|(i), are plotted against |z|(i) = Φ−1{(n + i −1/8)/(2n + 1/2)}. All outliers will then appear in the upper right of such a plot, as points separate from the trend. The normal-theory reference line, |r|(i) = |z|(i) and the normal-theory confidence interval can be replaced by simulating residuals from the assumed distribution, that need not be normal. The reference line is taken as the mean of S simulations and the interval with 1 − α coverage is taken as the ($\alpha$/2, 1 − $\alpha$/2) quantiles of their values.

Specifically, for a GLM, S sets of random observations yj , j = 1, 2, ... S are generated from the fitted model, each with mean µ hat, the fitted values under from the model and with the same distribution. In R, this is readily accomplished using the generic simulate() function; the random variation around µ hat uses rnorm(), rpois(), rnegbin(), etc., as appropriate for the family of the model.The same model is then fit to each simulated yj , giving a new set of residuals for each simulation. Sorting their absolute values then gives the simulation distribution used as reference for the observed residuals.

First, calculate the sorted absolute values of the residuals |r|(i) and their expected normal values, |z|(i) . The basic plot will be a  plot of expected v observed. Then, use simulate() to generate S = 100 simulated response vectors around the fitted values in the model. Here this uses the negative-binomial random number generator (rnegbin()) with the same dispersion value estimated in the model. The result, called sims here, is a data frame of n rows and S = 100 columns, named sim_1, sim_2, .... Next we have to fit the NB model S times and use the same model formula as the original, but with the simulated y. We first define a function to do this for a given y, and then use apply() to calculate them all. To save computing time, the coefficients from the original model are used as starting values. We then use apply() to compute the summary measures defining the center and limits for the simulated confidence interval. Finally, we plot the observed against expected absolute residuals as points, and add the lines for the confidence interval.

**Poisson paired counts**

An example of a paired study design in which the outcome measure is counts of events and which does not use a concurrent control, so is not a great study design. Nevertheless, we can adjust the total number of subjects assessed before and after an intervention. The Poisson model will be used, the observation time is the same for the the pre and post periods. We will simulate the data based on a expected rate before the intervention and an hypothesised effect of the intervention. We also take into account the correlation between observations for the same individual, the starting value is 0.2. 
 
Notes
===

gelmans book rnegbiniom function
 
predict

cor neg binomial not working

more refs


References
====
```{r}
 
    tags$a(href = "https://stats.stackexchange.com/questions/71194/fitting-a-poisson-distribution-with-lme4-and-nlme", target="_blank",tags$span(style="color:blue", "[1] Poisson with lme4 and nlme"),) 
    div(p(" "))
    tags$a(href = "https://stats.stackexchange.com/questions/27869/fitting-a-poisson-glm-mixed-model-with-a-random-slope-and-intercept",target="_blank",  tags$span(style="color:blue", "[2] Mixed model Poisson"),)
    div(p(" "))
    tags$a(href = "https://stackoverflow.com/questions/6044800/adding-greek-character-to-axis-title", target="_blank",tags$span(style="color:blue", "[3] Adding Greek characters - shiny"),)
    div(p(" "))
    tags$a(href = "https://stackoverflow.com/questions/52225348/shiny-flexdashboard-reference-object-in-new-tabset#52225733", target="_blank",tags$span(style="color:blue", "[4] Reference object in new tab - shiny"),)  
    div(p(" "))
    tags$a(href = "https://journals.sagepub.com/doi/pdf/10.1177/1536867X1201200202", target="_blank",tags$span(style="color:blue", "[5] What hypotheses do “nonparametric” two-group tests actually test?"),) 
    div(p(" "))
    tags$a(href = "https://www.google.co.uk/books/edition/Discrete_Data_Analysis_with_R/danODwAAQBAJ?hl=en",target="_blank",
           tags$span(style="color:blue", "[6] Discrete Data Analysis with R Visualization and Modeling Techniques for Categorical and Count Data"),) 
    div(p(" "))
    tags$a(href = "https://www.galitshmueli.com/system/files/ASMB_901_rev.pdf",target="_blank",
           tags$span(style="color:blue", "[7] On generating multivariate Poisson data in management science applications"),) 
    div(p(" "))
    tags$a(href = "https://thomasward.com/simulating-correlated-data/",target="_blank",tags$span(style="color:blue", "[8] Simulating correlated data"),) 
    div(p(" "))
    tags$a(href = "https://www.rdocumentation.org/packages/simstudy/versions/0.3.0", target="_blank", tags$span(style="color:blue", "[9] package simstudy for simulating correlated data, not used but interesting"),) 
    div(p(" "))
    tags$a(href = "https://www.rdocumentation.org/packages/SimCorrMix/versions/0.1.1/topics/SimCorrMix",target="_blank", tags$span(style="color:blue", "[10] package SimCorrMix for simulating correlated data, not used but interesting"),) 
      
    div(p(" "))
    tags$hr()                        

```