---
title: "Illustrating negative binomial distributions"
author: 
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
    social: menu
    source_code: embed
runtime: shiny
---
Home
===

```{r global, include=FALSE}

library(MASS) # for neg binomial analysis

lwd.=3

 my_dnbinom=function(x,mu,alpha){
  m = mu
  k = 1/alpha
  # lgamma is log(gamma)
  a = lgamma(k+x)-lgamma(k)-lgamma(x+1)
  b = x*log(m/(m+k))
  c = -k*log(1+m/k)
  prob = exp(a+b+c)
  prob[x<0] = 0
  prob[prob>1] = 1
  prob[prob<0] = 0
  return(prob)
}

# check
# dnbinom(   1, size=2,   mu=3)
# my_dnbinom(1, alpha=1/2,mu=3)

##################################################################################
# R has built-in random number generators for the binomial distribution,
# but they are expressed in different form than the form in the Lloyd-Smith
# PLoS paper. This function translates the mu and alpha parameters
# into the format needed by R to generate the NB random numbers
##################################################################################
my_rnbinom=function(n,mu,alpha){
  size = 1/alpha          # so now we can just enter alpha this is converted to 1/ alpha
  prob = size/(mu+size)   # so now we can enter mu the mean and that is converted to prob
  return(rnbinom(n,size=size,prob=prob))
}




nb.power <- function(n=220, k=1.3, mu0=1, eff=0.65, drop1=.1, drop2=.1, fup=1) {
  
  mu1 <- eff*mu0
  
  if (drop1==0) {drop1=0.0001}
  if (drop2==0) {drop2=0.0001}
  # n=220; alpha=1.3; mu0=1; mu1=.65; drop1=.01; drop2=.01; fup=1
 
  dose <- c(rep("placebo",n),rep("trt",n)) # 50:50 split of patients
  
  mu   <- c(rep(mu0,n), rep(mu1,n))    # rates in two arms

  drop <- c(rep(drop1,n), rep(drop2,n)) # }# tr discontinuation rates
  
  f    <- - rexp(2*n) / log(1-drop)
  
  length <- ifelse(f > fup, fup, f)        # curtail at follow up time 
  
  y  <-  rnbinom(n*2,  prob=1/(1+ mu*length* k),        size=1/k)  +   # on treatment events
         rnbinom(n*2,  prob=1/(1+ mu0*(fup-length)*k),  size=1/k)      # accounting for any off treat. 
  
  # assume no is lost to the study but can discontinue treatment
  lfup     <- log(fup)         # exposure log(fup) for everyone, that is fup year
  logleng  <- rep(lfup, n*2)   #  
  
  # analyse with neg. binomial model
  x <- summary(MASS::glm.nb(y~dose+offset((logleng))))
 
  # collect p-values
  p <-  x$coefficients["dosetrt","Pr(>|z|)"]
  
  return(p)
  
}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# correlated poisson function
 
library(lme4)
 
po.cor.power <- function(n=22, r=.75, mu0=10, eff.p=.75) { # r correlation mu0 placebo rate, mu1 expected change in rate
   
  # 1. create correlated poisson
  # 2. analyse many times and examine

  L1 <- mu0
  L2 <- mu0*eff.p
  
  # generate correlated poisson ref: https://thomasward.com/simulating-correlated-data/
  
  # Sample correlated N(0, 1) distributions from a multivariate normal distribution.
  # Transform them to correlated Uniform(0, 1) distributions with the normal CDF.
  # Transform them to any correlated probability distribution you desire with that probability distribution’s inverse CDF.
  
  Sigma <- matrix(c(1, r, r, 1), 2, 2)
  #Sigma
  
  mvrnorm <- function(n = 1, mu = 0, Sigma) {
    nvars <- nrow(Sigma)
    # nvars x n matrix of Normal(0, 1)
    nmls <- matrix(rnorm(n * nvars), nrow = nvars)
    # scale and correlate Normal(0, 1), "nmls", to Normal(0, Sigma) by matrix mult
    # with lower triangular of cholesky decomp of covariance matrix
    scaled_correlated_nmls <- t(chol(Sigma)) %*% nmls
    # shift to center around mus to get goal: Normal(mu, Sigma)
    samples <- mu + scaled_correlated_nmls
    # transpose so each variable is a column, not
    # a row, to match what MASS::mvrnorm() returns
    t(samples)
  }
  
  p2 <- mvrnorm(n, Sigma = Sigma)   # correlated continuous
  
  U <- pnorm(p2, mean = 0, sd = 1)  # correlated uniform
  
  pp1 <- qpois(U[, 1], L1)          # correlated poisson
  pp2 <- qpois(U[, 2], L2)          # correlated poisson
  
  # create a data frame
  my_data <- data.frame( 
    group = rep(c("before", "after"), each = n),
    counts = c(pp1,  pp2),
    ID=rep(1:n,2)
  )
  
  # analyse
  # https://stats.stackexchange.com/questions/71194/fitting-a-poisson-distribution-with-lme4-and-nlme
  # https://stats.stackexchange.com/questions/27869/fitting-a-poisson-glm-mixed-model-with-a-random-slope-and-intercept
   
  A <- glmer(counts ~ group + (1|ID), data=my_data, family="poisson")
  B <- glmer(counts ~ 1     + (1|ID), data=my_data, family="poisson")
  p <- anova(A,B)
  mix <-  p$`Pr(>Chisq)`[2]
  
  x <- summary(A)
  rate.reduction <- 1-1/exp(x$coefficients[2,1])
  
  intercept <- exp( x$coefficients[1,1])
  beta      <- exp( x$coefficients[2,1])
  
  x1 <- pp2 - pp1                                     # post - pre
  
  wil <- wilcox.test(x=x1, paired=FALSE)$p.value
  
  t.t <- t.test(x=x1, paired=FALSE)$p.value 
  
  ttor <- t.test(x=rank(x1), paired=FALSE)$p.value 
  
  #lets capture SD of differences
  sdd <- sd(x1)
  
  #newList <- list("glmer" = mix , "Wilcoxon signed rank test" = wil,
              #    "t.test"=t.t, "rate reduction"= rate.reduction, "intercept"=intercept, "beta"=beta,
               #   "ttest on ranks"=ttor, "sd of diff"=sdd)
  #return(newList)
  
  
  
  c(rate.reduction,mix , wil,t.t,  ttor,intercept, beta ,sdd)
  
}


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


```


Column {.sidebar}
-----------------------------------------------------------------------

Count data analysis is explored...

```{r}

library(shiny)
library(utf8)

sliderInput('mu', 'Common  \u03BC', value=1.5,
              min = 1, max = 25, step=.5, ticks=F)

sliderInput('alpha_a', '\u03B1 for red curve', value = c(0.8),
              min = 0, max = 5 ,step=0.01, ticks=F)

sliderInput('alpha_b', '\u03B1 for dark blue curve', value = c(0.8),
              min = 0, max = 5 ,step=0.01, ticks=F)

sliderInput('alpha_d', '\u03B1 for purple curve', value = c(0.8),
              min = 0, max = 5 ,step=0.01, ticks=F)

sliderInput('sims', 'Simulations (chart 2)', 50000,
              min = 50000, max = 500000,step=50000,ticks=F)

sliderInput('x_range', 'Plot x-range', value = c(0,10),
        min = 0, max = 80,step=5,ticks=F)

sliderInput('y_range', 'Plot y-range', value = c(0,.4),
        min = 0, max = 1,step=.05,ticks=F)

 

```

This page of the app uses  $\alpha$. 

$\alpha$ takes on positive rational values, rarely above 4 (Hilbe page 190)

When  $\alpha$ approaches 0, k is infinite and Poisson emerges (move red $\alpha$ slider to 0).

When  $\alpha$ > 1, k < 1 highly leads to over dispersed data ($\alpha$ big).

$\alpha$ = 1/k

k = 1/ $\alpha$
 

Column {data-width=400, height=300}
-----------------------------------------------------------------------
### Chart 1


```{r}

renderPlot({
  
 
require("sfsmisc")
#set.seed(28754) # set random number seed for reproducibilty of results

##################################################################################
# Define the Negative Binomial probability density function, with parameters
# mu and alpha
# The mean of this NB distribution is mu, and variance is sigma^2=mu+alpha*mu^2
##################################################################################

##################################################################################
# Now let's try out the NB density function, and overlay the Poisson
# distribution with the same mean
##################################################################################
#mult.fig(2) # divide the plotting area up into 2 (one up and one down)

# mu = sample(1:25,1)
# alpha_a = 0.1
# alpha_b = .01
# alpha_d = 1

  mu = input$mu
  alpha_a = input$alpha_a
  alpha_b =  input$alpha_b
  alpha_d =  input$alpha_d
  
  low =  input$x_range[1]
  high=  input$x_range[2]
  xmax = high
  
  ymax = input$y_range[2]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  # if else turn neg binomial into poisson if alpha =0
  x=c(-0.0001,seq(0,xmax,1))
  
  y =          dpois(x,mu)
   
  if(alpha_a==0) {
  yb =          dpois(x,mu)
  }else{
  yb=     my_dnbinom(x,mu,alpha_a)
  }
  
  if(alpha_b==0){
  yc =          dpois(x,mu)
  }else{
  yc=     my_dnbinom(x,mu,alpha_b)
  }
  
  if(alpha_d==0){
  yd =          dpois(x,mu)
  }else{
  yd=     my_dnbinom(x,mu,alpha_d)
  }
  
  nor <-       dnorm(x, mu, sqrt(mu))


  x=append(0,x)  
  y=append(0,y)
  yb=append(0,yb)
  yc=append(0,yc)
  yd=append(0,yd)
  nor=append(0,nor)
  
  l = !is.na(y+yb+yc+yd+nor)
  
  plot(x[l],y[l],ylim=c(0,ymax),xlim=c(low,xmax),type="l",lwd=lwd.,xlab="x",
       ylab="prob(x)",main="Probability distributions")
  
  lines(x[l],yb[l], col=2,lwd=lwd.,type="l")
  lines(x[l],yc[l], col=4,lwd=lwd.,type="l")
  lines(x[l],nor[l],col=5,lwd=lwd.,type="l")
  lines(x[l],yd[l], col=6,lwd=lwd.,type="l")
   
  # https://stackoverflow.com/questions/6044800/adding-greek-character-to-axis-title
  apois = paste("Poisson \u03BC=",mu,sep="")
  anegbinom_a = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_a,sep="")
  anegbinom_b = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_b,sep="")
  anegbinom_c = paste("Normal \u03BC=",   mu," \u03C3=sqrt(",mu,")",sep="")
  anegbinom_d = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_d,sep="")
  
  
  legend("topright",col=c(1,2,4,6,5),legend=c(apois,anegbinom_a,anegbinom_b, anegbinom_d, anegbinom_c),
         lwd=lwd.,bty="n")
 }
)




```


### Chart 2

```{r}

renderPlot({
  

    n=input$sims
    
    mu = input$mu
    alpha_a =  input$alpha_a
    alpha_b =  input$alpha_b
    alpha_d =  input$alpha_d
    
    low =  input$x_range[1]
    high=  input$x_range[2]
    xmax = high
    
    ymax = input$y_range[2]
    
    z  =      rpois(n,mu)
    zb = my_rnbinom(n,mu,alpha_a)
    zc = my_rnbinom(n,mu,alpha_b)
    zd =      rnorm(n,mu,sqrt(mu))
    ze = my_rnbinom(n,mu,alpha_d)
    
    a=hist(z, plot=F,breaks=seq(-1.5,n+.5,1))
    b=hist(zb,plot=F,breaks=seq(-1.5,n+.5,1))
    c=hist(zc,plot=F,breaks=seq(-1.5,n+.5,1))
    d=hist(zd,plot=F,breaks=200)
    e=hist(ze,plot=F,breaks=seq(-1.5,n+.5,1))
    
    atitle = paste("Distributions of",n,"random numbers")
    
    plot(a$mids,a$density,type="s",lwd=lwd.,col=1,ylim=c(0,ymax),main=atitle,xlim=c(low,xmax),
         xlab="x",ylab="Fraction falling within each bin")
    
    lines(b$mids,b$density,type="s",lwd=lwd.,col=2)
    lines(c$mids,c$density,type="s",lwd=lwd.,col=4)
    lines(d$mids,d$density,type="s",lwd=lwd.,col=5)
    lines(e$mids,e$density,type="s",lwd=lwd.,col=6)
    
    apois = paste("Poisson \u03BC=",mu,sep="")
    anegbinom_a = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_a,sep="")
    anegbinom_b = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_b,sep="")
    anegbinom_c = paste("Normal \u03BC=",   mu," \u03C3=sqrt(",mu,")",sep="")
    anegbinom_d = paste("Neg Binom \u03BC=",mu," \u03B1=",alpha_d,sep="")
    
    legend("topright",col=c(1,2,4,6,5),legend=c(apois,anegbinom_a,anegbinom_b, anegbinom_d, anegbinom_c),
           lwd=lwd.,bty="n")


})


 
```

Column {data-width=400}
-------------------------------------

### Chart 3

```{r}


renderPlot({
  
    mu = input$mu
  alpha_a = input$alpha_a
  
  
  low =  input$x_range[1]
  high=  input$x_range[2]
  xmax = high
  
  ymax = input$y_range[2]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 
  xx <- c(seq(0,xmax,1))

  yb = dpois(xx,mu)
  
  par(mar=c(2.1,4.1,4.1,4.1))
      
  barplot(yb, xlim=c(0,high+2), ylab = "Percent", xlab="x", col='black', ylim=c(0,ymax),
          main = paste("Poisson \u03BC=", mu, ", based on common \u03BC",sep=""),
          names.arg= as.character(xx))
})

```

### Chart 4

```{r}

 
    renderPlot({
  
    mu = input$mu
  alpha_a = input$alpha_a
  
  
  low =  input$x_range[1]
  high=  input$x_range[2]
  xmax = high
  
  ymax = input$y_range[2]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 
  xx <- c(seq(0,xmax,1))

  if(alpha_a==0) {
  yb =          dpois(xx,mu)
  }else{
  yb=     my_dnbinom(xx,mu,alpha_a)
  }
  
  par(mar=c(2.1,4.1,4.1,4.1))
      
  barplot(yb, xlim=c(0,high+2), ylab = "Percent", xlab="x", col='red',
          ylim=c(0,ymax),
          main =  paste("Negative Binomial \u03BC=",mu," \u03B1=",alpha_a,", based on common mean and red curve \u03B1",sep=""),
          names.arg= as.character(xx))
  
})
    
    
   


```

Notes
===

When $\alpha$ = 1, the negative binomial distribution takes the form of a
geometric distribution, which is the discrete correlate of the continuous negative
exponential distribution. 
The negative binomial distribution with $\alpha$ = 0 is Poisson.
As the mean increases, the probability of
a zero decreases, and the more the shape approximates a Gaussian distribution.
As the Poisson mean gets larger it approaches normality, the negative binomial distribution does not respond to larger mean values in that manner. 


The binomial distribution describes the number of r successes in n trials.

The geometric distribution describes the number of failures before the first success, r.

The negative binomial distribution describes the number of failures before the rth success. (Hilbe page 194)

Confusingly, the term ‘dispersion parameter’ can refer to either k or $\alpha$ ; other terms for k include ‘shape
parameter’ and ‘clustering coefficient’.

The only restriction placed on  $\alpha$ is that it take positive rational values,
rarely above 4 (Hilbe page 190)

 

Underdispersed data are assigned the minimum value of $\alpha$, corresponding to k going to infinity.



Accordingly, all calculations in this study were conducted using $\alpha$, but all results and discussion
are posed in terms of k?

In this study ML estimation was conducted for the parameter $\alpha$,
but results are reported in terms of k = 1/ $\alpha$ because k is more
familiar to epidemiologists and ecologists. Estimates of $\alpha$ were
restricted to positive values, because the allowed range for k was (0,infinity). 


file:///C:/Users/Lenovo/OneDrive/Documents/PAPERS/COUNT%20DATA/TRISTAN%20study%20Keene%202007.pdf
thy dispersion is alpha in this paper example figure 1

 The larger the gamma shape parameter, the more dispersed is the distribution
 
 The dispersion parameter k is commonly used as an inverse measure of aggregation in biological count data



Power
====


Column {.sidebar}
-----------------------------------------------------------------------



```{r}

 
# jscode <- "shinyjs.refresh = function() { history.go(0); }"
# 
# actionButton(jscode, "Refresh")
# 
# observeEvent(input$reset, {
# 
# })

 
 

sliderInput('N', 'Patients per arm', value=220,
              min = 10, max = 10000, step=5, ticks=F)

sliderInput('mu0', 'Placebo mean rate \u03BC', value=1,
              min = 0.1, max = 25, step=.05, ticks=F)

sliderInput('eff', 'Hypothesised treatment effect ', value=.75,
              min = 0.1, max = 25, step=.05, ticks=F)

sliderInput('k', '1/\u03B1 the heterogeneity (ancillary) parameter', value = c(1.3),
              min = 0, max = 5 ,step=0.01, ticks=F)

sliderInput('d1', 'Placebo discontinuation prob', value = c(0.1),
              min = 0, max = .5 ,step=0.05, ticks=F)

sliderInput('d2', 'treatment discontinuation prob', value = c(0.1),
              min = 0, max = .5 ,step=0.05, ticks=F)

sliderInput('fup', 'Follow up (yrs)', value = c(1),
        min = 0, max = 10, step=1,ticks=F)
 
sliderInput('sims2', 'Power simulations', value= 500,
              min = 500, max = 10000, step= 500, ticks=F)
 
 

```

Row {data- height=650}
-----------------------------------------------------------------------
### Power will appear here iminently...be patient...
  
```{r, eval=TRUE}

reactive({
  
cat(paste0("A RCT with 1:1 randomisation is being planned to investigate the rate of asthma exacerbations with ",input$N," patients per arm.\n"))

cat(paste0("The negative binomial model will be used as patient heterogeneity of exacerbations beyond that captured by patient level covariates is expected.\nWe will use the natural log of duration of follow up time as an offset variable, the duration of follow up, ",input$fup, " will be the planned duration of follow up for all patients (see slider)."))

cat(paste0("\nk is the dispersion parameter value of " ,input$k," this is the same as 1/alpha So alpha is ",1/input$k,". The rate for placebo is ",input$mu0," events per patient year of follow up.\nThe treatment effect hypothesised is  ",input$eff, ". \nDrop out probabilities for treatment discontinuation and follow up period (in years) are " ,input$d1, " and " ,input$d2," respectively. \nWe assume a constant exponential hazard rate over time for treatment discontinuation with ",input$d1," and ",input$d2,", the percentages of patients\noff treatment by follow up time in years in active and placebo. \nNo further treatment effect versus placebo is assumed during off treatment, that is ",input$mu0," events per patient year is assumed for patients after treatment discontinuation in treated and placebo.\n"))

cat(paste0("\nThe power estimate is..."))
})
  
 

reactive({
  
res <- replicate( input$sims2, 
      nb.power(n=input$N, k=input$k, mu0=input$mu0, eff=input$eff, drop1=input$d1, drop2=input$d2, fup=input$fup)) 
mean(res<0.05)

})


```
 
Row {data-height=350}
-------------------------------------
   
### A typical dataset that can be expected

```{r}

 renderPlot({
  
 n=input$N
 k=input$k
 mu0=input$mu0
 eff=input$eff
 drop1=input$d1
 drop2=input$d2
 fup=input$fup
 
 mu1 <- eff*mu0
 
  dose <- c(rep("placebo",n),rep("trt",n))
  mu   <- c(rep(mu0,n),rep(mu1,n))
  if (drop1==0) {drop1=0.0001}
  if (drop2==0) {drop2=0.0001}
  drop <- c(rep(.1,n),rep(.1,n))

  f <- - rexp(2*n) / log(1-drop)
  length <- ifelse(f>1,1,f)
 
  y <-  rnbinom(n*2,  p=1/(1+ mu*length* k),      size=1/k)  +
        rnbinom(n*2,  p=1/(1+ mu0*(1-length)*k),  size=1/k)

  logleng  <- rep(0, n*2)

  #addmargins(table( y,  dose))

 # d <- cbind.data.frame(dose, mu, length, y, logleng)
#  summary(MASS::glm.nb(y~dose+offset((logleng)), data=d))

  trt <- y[dose %in% "trt"]
  pla <- y[!dose %in% "trt"]
# examine
# par(mfrow=c(1,2))
# trt %>% table %>% barplot() #quick and dirty
# trt %>% table %>% prop.table
# pla %>% table %>% barplot() #quick and dirty
# pla %>% table %>% prop.table
# par(mfrow=c(1,1))
upp <- ceiling(max(prop.table(table(y))) * 100 ) 
par(mfrow=c(1,2))
data_perc <- t(prop.table(table(trt))) * 100    # Convert data to probability table
barplot(data_perc, ylab = "Percent", main ="Treated", ylim=c(0,upp ))     
data_perc <- t(prop.table(table(pla))) * 100    # Convert data to probability table
barplot(data_perc, ylab = "Percent", main ="Placebo", ylim=c(0,upp ))     
par(mfrow=c(1,1))

})



```   
 
   
### A typical result that can be expected, theta is synonymous with $\alpha$

```{r}

 renderPrint({
  
 n=input$N
 k=input$k
 mu0=input$mu0
 eff=input$eff
 drop1=input$d1
 drop2=input$d2
 fup=input$fup
 
 mu1 <- eff*mu0
 
  dose <- c(rep("placebo",n),rep("trt",n))
  mu   <- c(rep(mu0,n),rep(mu1,n))
  if (drop1==0) {drop1=0.0001}
  if (drop2==0) {drop2=0.0001}
  drop <- c(rep(.1,n),rep(.1,n))

  f <- - rexp(2*n) / log(1-drop)
  length <- ifelse(f>1,1,f)
 
  y <-  rnbinom(n*2,  p=1/(1+ mu*length* k),      size=1/k)  +
        rnbinom(n*2,  p=1/(1+ mu0*(1-length)*k),  size=1/k)

  logleng  <- rep(0, n*2)

  #addmargins(table( y,  dose))

  d <- cbind.data.frame(dose, mu, length, y, logleng)
  print (f <- summary(MASS::glm.nb(y~dose+offset((logleng)), data=d)))
  
cat(paste0("\nPlacebo rate ",round(exp(f$coef[1]),4)," and multiplicative treatement effect of ",round(exp(f$coef[2]),4),""))
    
})
  
``` 

Paired counts
====

Column {.sidebar}
-----------------------------------------------------------------------

```{r}


sliderInput('N.p', 'Patients per arm', value=22,
              min = 10, max = 10000, step=5, ticks=F)

sliderInput('mu0.p', 'Placebo mean rate \u03BC', value=10,
              min = 0.1, max = 25, step=.05, ticks=F)

sliderInput('eff.p', 'Hypothesised treatment effect ', value=.75,
              min = 0.1, max = 25, step=.05, ticks=F)

sliderInput('r', 'Correlation', value = c(.5),
              min = 0, max = 1 ,step=0.05, ticks=F)
 
sliderInput('sims.p', 'Power simulations', value= 10,
              min = 10, max = 10000, step= 500, ticks=F)
 

```

Row {data-height=650}
-----------------------------------------------------------------------

### xxxxxxxxxxxxxxx
  

```{r, eval=TRUE}

 

reactive({
  
  res <- replicate(input$sims.p, 
                   po.cor.power(n=input$N.p, r=input$r, mu0=input$mu0.p, eff.p=input$eff.p ) )  

  #print(res)
 x <- t((res))
  x <- as.data.frame(x)
  alpha=0.05
  print(mean( unlist(x[,"V1"]))   )  
  print(median( unlist(x[,"V2"]))  )                                      # median mix p-value
  print(table( unlist(x[,"V2"])< alpha)/input$sims.p)                     # power mix model
  print(table( unlist(x[,"V3"])< alpha)/input$sims.p )                      # Wilcoxon signed rank test power
  print(table( unlist(x[,"V4"])< alpha)/input$sims.p )                  # ttest power
  print(table( unlist(x[,"V5"])< alpha)/input$sims.p )        # ttest on ranks power
  print(mean( unlist(x[,"V6"])) )  # intecept
  print(mean( unlist(x[,"V7"]))  )    # beta
  print(mean( unlist(x[,"V8"])) )  # sd of diff
  # theory
  R <- seq(-.9,.9,.1)   # correlation
  X <- 10             # var of poisson (= mean)
  Y <- 7.5           # var of poisson (= mean)
  print(cbind(R,   sqrt(X+Y-(2*sqrt(X)*sqrt(Y)*R))))
 
  
  
 # c(rate.reduction,mix , wil,t.t,  ttor,intercept, beta ,sdd)

})

```


References
====

```

